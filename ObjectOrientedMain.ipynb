{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SentimentAnalysis(object):\n",
    "   \n",
    "    def __init__(self,train, test):\n",
    "        self.train=train\n",
    "        self.test=test\n",
    "        self.logreg=LogisticRegression( \n",
    "                            dual=False, tol=0.0001,fit_intercept=True, intercept_scaling=1, \n",
    "                            class_weight=None, random_state=None,\n",
    "                            solver='newton-cg',multi_class='multinomial', verbose=0, \n",
    "                            warm_start=False, n_jobs=None)\n",
    "               \n",
    "        self.countvectorizer=CountVectorizer(\n",
    "                                    input='content',\n",
    "                                    encoding='utf-8',\n",
    "                                    decode_error='strict',\n",
    "                                    strip_accents=None,\n",
    "                                    lowercase=True,\n",
    "                                    preprocessor=None,\n",
    "                                    tokenizer=None,\n",
    "                                    stop_words=None,\n",
    "                                    token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                                    ngram_range=(1,2),\n",
    "                                    analyzer='word',\n",
    "                                    max_df=1.0,\n",
    "                                    min_df=1,\n",
    "                                    max_features=None,\n",
    "                                    vocabulary=None,\n",
    "                                    binary=False,\n",
    "                                    dtype=np.float64\n",
    "                                    )\n",
    "        \n",
    "        self.xgboost = XGBClassifier(\n",
    "                       booster='gbtree', colsample_bylevel=1,base_score=0.5,learning_rate=0.5,\n",
    "                       colsample_bytree=1, gamma=0, max_delta_step=0,\n",
    "                       max_depth=8, min_child_weight=1, missing=None, \n",
    "                       n_jobs=1, nthread=4, objective='multi:softmax',num_class=5, random_state=0,\n",
    "                       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "                       silent=True, subsample=1)\n",
    "        \n",
    "        self.naive = MultinomialNB()\n",
    "    \n",
    "    def featuring(self):\n",
    "        merge=pd.concat([self.train.iloc[:,0:3],self.test.iloc[:,0:3]],  sort = False)\n",
    "        df=merge.reset_index(drop=True)\n",
    "        corpus=merge.Phrase\n",
    "        COMMENT = 'Phrase'\n",
    "        df['total_length'] = df[COMMENT].apply(len)\n",
    "\n",
    "        df['words'] = df[COMMENT].apply(lambda comment: len(comment.split()))\n",
    "        df['words_vs_length'] = df['words'] / df['total_length']\n",
    "\n",
    "        df['capitals'] = df[COMMENT].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "        df['capitals_vs_length'] = df['capitals'] / df['total_length']\n",
    "\n",
    "\n",
    "        df['paragraphs'] = df[COMMENT].apply(lambda comment: comment.count('\\n'))\n",
    "        df['paragraphs_vs_length'] = df['paragraphs'] / df['total_length']\n",
    "\n",
    "\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "        df['stopwords'] = df[COMMENT].apply(lambda comment: sum(comment.count(w) for w in eng_stopwords))\n",
    "        df['stopwords_vs_length'] = df['stopwords'] / df['total_length']\n",
    "\n",
    "\n",
    "        df['exclamation_marks'] = df[COMMENT].apply(lambda comment: comment.count('!'))\n",
    "        df['exclamation_marks_vs_length'] = df['exclamation_marks'] / df['total_length']\n",
    "\n",
    "\n",
    "        df['question_marks'] = df[COMMENT].apply(lambda comment: comment.count('?'))\n",
    "        df['question_marks_vs_length'] = df['question_marks'] / df['total_length']\n",
    "\n",
    "\n",
    "        df['unique_words'] = df[COMMENT].apply(\n",
    "            lambda comment: len(set(w for w in comment.split())))\n",
    "        df['unique_words_vs_length'] = df['unique_words'] / df['total_length']\n",
    "\n",
    "\n",
    "        import collections\n",
    "\n",
    "        repeated_threshold = 15\n",
    "        def count_repeated(text):\n",
    "            text_splitted = text.split()\n",
    "            word_counts = collections.Counter(text_splitted)\n",
    "            return sum(count for word, count in sorted(word_counts.items()) if count > repeated_threshold)\n",
    "\n",
    "\n",
    "\n",
    "        df['repeated_words'] = df[COMMENT].apply(lambda comment: count_repeated(comment))\n",
    "        df['repeated_words_vs_length'] = df['repeated_words'] / df['total_length']\n",
    "\n",
    "\n",
    "        def tag_part_of_speech(text):\n",
    "            text_splited = text.split(' ')\n",
    "            text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "            text_splited = [s for s in text_splited if s]\n",
    "            pos_list = pos_tag(text_splited)\n",
    "            noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
    "            adjective_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
    "            verb_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
    "            return[noun_count, adjective_count, verb_count]\n",
    "\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "        df['nouns'], df['adjectives'], df['verbs'] = zip(*df[COMMENT].apply(\n",
    "            lambda comment: tag_part_of_speech(comment)))\n",
    "\n",
    "        df['nouns_vs_length'] = df['nouns'] / df['total_length']\n",
    "        df['adjectives_vs_length'] = df['adjectives'] / df['total_length']\n",
    "        df['verbs_vs_length'] = df['verbs'] / df['total_length']\n",
    "        #Scaling the features\n",
    "        num_features = [f_ for f_ in df.columns\n",
    "                if f_ not in [\"Phrase\", \"PhraseId\",\"SentenceId\"]]\n",
    "\n",
    "        type(num_features)\n",
    "        print(num_features)\n",
    "        skl = MinMaxScaler()\n",
    "        df_num_features = skl.fit_transform(df[num_features])\n",
    "        print(df_num_features.shape)\n",
    "        #Separating train, test data\n",
    "        self.train_num_features = df_num_features[:train.shape[0]]\n",
    "        self.test_submit_num_features = df_num_features[train.shape[0]:]\n",
    "\n",
    "        \n",
    "    def countvect(self):\n",
    "        self.train['Phrase'] = self.train['Phrase'].str.lower().replace('[^a-zA-Z0-9]', ' ', regex = True)\n",
    "        self.test['Phrase'] = self.test['Phrase'].str.lower().replace('[^a-zA-Z0-9]', ' ', regex = True)\n",
    "        self.train_vect = self.countvectorizer.fit_transform(train['Phrase'])\n",
    "        self.test_vect = self.countvectorizer.transform(test['Phrase'])\n",
    "        self.X_train = hstack([self.train_vect, self.train_num_features])\n",
    "        self.X_test_submit = hstack([self.test_vect, self.test_submit_num_features])\n",
    "\n",
    "        \n",
    "    def trainLogistic(self):\n",
    "        grid={\"C\":[1.0], \"max_iter\":[100]}\n",
    "        self.GridSe1 = GridSearchCV(self.logreg , grid, cv=25)\n",
    "        self.GridSe1.fit(self.X_train, self.train['Sentiment'])\n",
    "        print(\"tuned hpyerparameters :(best parameters) \",self.GridSe1.best_params_)\n",
    "        print(\"accuracy :\",self.GridSe1.best_score_)\n",
    "        LogRegGrid_path = 'LogRegGrid.pkl'\n",
    "        # Create an variable to pickle and open it in write mode\n",
    "        LogRegGrid_pickle = open(LogRegGrid_path, 'wb')\n",
    "        pickle.dump(self.GridSe1, LogRegGrid_pickle)\n",
    "        LogRegGrid_pickle.close()\n",
    "\n",
    "    def testLogistic(self):\n",
    "        self.pred1=self.GridSe1.predict(self.X_test_submit)\n",
    "        first_sub=self.test.copy()\n",
    "        n=first_sub.shape[0]\n",
    "        first_sub[\"Sentiment\"] = self.pred1\n",
    "        first_sub = first_sub.loc[:, [\"PhraseId\", \"Sentiment\"]]\n",
    "        first_sub.to_csv(\"logistic.csv\", index=False)\n",
    "        \n",
    "    def trainxgboost(self):\n",
    "        grid={\"n_estimators\":[1000]}\n",
    "        self.GridSe2 = GridSearchCV(self.xgboost , grid, cv=25)\n",
    "        self.GridSe2.fit(self.X_train, self.train['Sentiment'])\n",
    "        print(\"tuned hpyerparameters :(best parameters) \",self.GridSe2.best_params_)\n",
    "        print(\"accuracy :\",self.GridSe2.best_score_)\n",
    "        XGBoost_path = 'XgBoost.pkl'\n",
    "        # Create an variable to pickle and open it in write mode\n",
    "        XGBoost_pickle = open(XGBoost_path, 'wb')\n",
    "        pickle.dump(self.GridSe2, XGBoost_pickle)\n",
    "        XGBoost_pickle.close()\n",
    "        \n",
    "    def testxgboost(self):\n",
    "        self.pred2=self.GridSe2.predict(self.X_test_submit)\n",
    "        first_sub=self.test.copy()\n",
    "        n=first_sub.shape[0]\n",
    "        first_sub[\"Sentiment\"] = self.pred2\n",
    "        first_sub = first_sub.loc[:, [\"PhraseId\", \"Sentiment\"]]\n",
    "        first_sub.to_csv(\"xgboost.csv\", index=False)\n",
    "        \n",
    "        \n",
    "    def trainnaive(self):\n",
    "        grid={\"alpha\":[0.001]}\n",
    "        self.GridSe3 = GridSearchCV(self.naive , grid, cv=25)\n",
    "        self.GridSe3.fit(self.X_train, self.train['Sentiment'])\n",
    "        print(\"tuned hpyerparameters :(best parameters) \",self.GridSe3.best_params_)\n",
    "        print(\"accuracy :\",self.GridSe3.best_score_)\n",
    "        NaiveBayes_path = 'MultinomialNB.pkl'\n",
    "        # Create an variable to pickle and open it in write mode\n",
    "        NaiveBayes_pickle = open(NaiveBayes_path, 'wb')\n",
    "        pickle.dump(self.GridSe3, NaiveBayes_pickle)\n",
    "        NaiveBayes_pickle.close()\n",
    "        \n",
    "    def testnaive(self):\n",
    "        self.pred3=self.GridSe3.predict(self.X_test_submit)\n",
    "        first_sub=self.test.copy()\n",
    "        n=first_sub.shape[0]\n",
    "        first_sub[\"Sentiment\"] = self.pred3\n",
    "        first_sub = first_sub.loc[:, [\"PhraseId\", \"Sentiment\"]]\n",
    "        first_sub.to_csv(\"NaiveBayes.csv\", index=False)\n",
    "        \n",
    "        \n",
    "    def visualize(self):\n",
    "        dist = self.train.groupby([\"Sentiment\"]).size()\n",
    "        dist = dist / dist.sum()\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        sns.barplot(dist.keys(), dist.values);\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test  = pd.read_csv(\"test.csv\")\n",
    "    model=SentimentAnalysis(train,test)\n",
    "    model.visualize()\n",
    "    model.featuring()\n",
    "    model.countvect()\n",
    "    \n",
    "    #LogisticRegression\n",
    "    model.trainLogistic()\n",
    "    model.testLogistic()\n",
    "    \n",
    "    #XG Boost\n",
    "    model.trainxgboost()\n",
    "    model.testxgboost()\n",
    "    \n",
    "    #Multinomial NaiveBayes\n",
    "    model.trainnaive()\n",
    "    model.testnaive()\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
